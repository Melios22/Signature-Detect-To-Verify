{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the necessary libraries and define the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5887e5a0-e85f-491f-b5a6-a4a38529aed6",
    "_uuid": "4d239c9f-3840-417e-bed0-5a5c4b3c67ba",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:00.501302Z",
     "iopub.status.busy": "2025-04-18T10:26:00.501014Z",
     "iopub.status.idle": "2025-04-18T10:26:03.977658Z",
     "shell.execute_reply": "2025-04-18T10:26:03.976760Z",
     "shell.execute_reply.started": "2025-04-18T10:26:00.501271Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install ultralytics huggingface_hub[hf_xet]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cff269cf-fbd4-4193-9b96-6bf2e2cce2a4",
    "_uuid": "1839e47f-54d6-4732-a947-cab88dc3b6f1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:03.978781Z",
     "iopub.status.busy": "2025-04-18T10:26:03.978515Z",
     "iopub.status.idle": "2025-04-18T10:26:06.924044Z",
     "shell.execute_reply": "2025-04-18T10:26:06.923298Z",
     "shell.execute_reply.started": "2025-04-18T10:26:03.978748Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SIGN_IMG_SIZE = (105, 105) # The image size to be fed into the Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Define the structure for the Siamese Model before loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ace0ddf-83ba-4911-b174-cb711bc27e9c",
    "_uuid": "ad920a09-7ac8-40a9-a23f-0e4380f5700c",
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:06.925284Z",
     "iopub.status.busy": "2025-04-18T10:26:06.924884Z",
     "iopub.status.idle": "2025-04-18T10:26:06.930966Z",
     "shell.execute_reply": "2025-04-18T10:26:06.930290Z",
     "shell.execute_reply.started": "2025-04-18T10:26:06.925261Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for similarity learning.\n",
    "    This loss function is particularly useful when training models to learn embeddings\n",
    "    where similar pairs of inputs are mapped close together in the embedding space,\n",
    "    and dissimilar pairs are mapped far apart.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.5):\n",
    "        \"\"\"\n",
    "        Initializes the ContrastiveLoss module.\n",
    "\n",
    "        Args:\n",
    "            margin (float, optional): The margin value for the contrastive loss.\n",
    "                                     It defines the boundary beyond which dissimilar pairs\n",
    "                                     should not contribute to the loss. Defaults to 1.5.\n",
    "        \"\"\"\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"\n",
    "        Computes the contrastive loss.\n",
    "\n",
    "        Args:\n",
    "            output1 (torch.Tensor): Output embeddings from the first input in the pair.\n",
    "                                    Shape: (batch_size, embedding_dimension)\n",
    "            output2 (torch.Tensor): Output embeddings from the second input in the pair.\n",
    "                                    Shape: (batch_size, embedding_dimension)\n",
    "            label (torch.Tensor): Labels indicating whether the pair is similar or dissimilar.\n",
    "                                  - 0 indicates a similar pair (should be close in embedding space).\n",
    "                                  - 1 indicates a dissimilar pair (should be far apart in embedding space).\n",
    "                                  Shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed contrastive loss. A scalar value representing the average loss\n",
    "                          over the batch.\n",
    "        \"\"\"\n",
    "        # Calculate the Euclidean distance between the two output embeddings for each pair in the batch.\n",
    "        # Computes the pairwise Euclidean distance between rows of two tensors.\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "        # Compute the contrastive loss.\n",
    "        # The loss has two components based on the label:\n",
    "        # 1. For similar pairs (label == 0):\n",
    "        #    - We want to minimize the squared Euclidean distance between the embeddings.\n",
    "        # 2. For dissimilar pairs (label == 1):\n",
    "        #    - We want to maximize the distance between the embeddings, but only up to a certain margin.\n",
    "        #    - If the Euclidean distance is greater than the margin, the loss contribution for this pair should be zero.\n",
    "        #    - label will be 1 for dissimilar pairs and 0 for similar pairs, effectively selecting this term\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc9d8150-c1df-4361-aa8f-3fc119182a9f",
    "_uuid": "01934e94-6456-42fa-9f89-1b7f52a1c6c1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:06.932092Z",
     "iopub.status.busy": "2025-04-18T10:26:06.931829Z",
     "iopub.status.idle": "2025-04-18T10:26:06.950855Z",
     "shell.execute_reply": "2025-04-18T10:26:06.950299Z",
     "shell.execute_reply.started": "2025-04-18T10:26:06.932053Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SiameseModel(nn.Module):\n",
    "    \"\"\"Siamese network for signature verification.\"\"\"\n",
    "    def __init__(self, lr=1e-4, from_file=None, embedding_size=256, margin=1.5):\n",
    "        \"\"\"\n",
    "        Initializes the SignatureRCNN model.\n",
    "\n",
    "        Args:\n",
    "            device (torch.device, optional): Device to use for computation (CPU or CUDA). Defaults to CUDA if available, otherwise CPU.\n",
    "            lr (float, optional): Learning rate for the optimizer. Defaults to 1e-4.\n",
    "            from_file (str, optional): Path to a pre-trained model file to load. Defaults to None.\n",
    "            embedding_size (int): Size of the final feature vector.\n",
    "        \"\"\"\n",
    "        super(SiameseModel, self).__init__()\n",
    "\n",
    "        ## Define this if you want to train the model again with the train() function\n",
    "        # self.criterion = ContrastiveLoss(margin=margin)\n",
    "        # self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        # Define convolutional layers for feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: 1 input channel (grayscale), 64 output channels\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(), # Replace ReLU\n",
    "            nn.BatchNorm2d(64), # Add BatchNorm to stabalize\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1), # Stride=2 for downsampling, (replaces MaxPool)\n",
    "\n",
    "            # Block 2: 64 input channels, 128 output channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Block 3: 128 input channels, 256 output channels\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Block 4: 256 input channels, 512 output channels\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1)) # Replace flattening, less overfit, more intepretable features\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "        # Load the model weight if exist\n",
    "        if from_file and os.path.exists(from_file):\n",
    "            self.load_from_file(from_file)\n",
    "        \n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        \"\"\"Pass input through the feature extractor.\"\"\" \n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"Compute embeddings for both images in the pair.\"\"\"\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        \"\"\"Load back the model weights from the .pt file\"\"\"\n",
    "        checkpoint = torch.load(file_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "        self.cnn.load_state_dict(checkpoint['cnn'])\n",
    "        self.fc.load_state_dict(checkpoint['fc'])\n",
    "\n",
    "    def save_to_file(self, file_path):\n",
    "        \"\"\"Save the model weights to the .pt file\"\"\"\n",
    "        torch.save({\n",
    "            'cnn': self.cnn.state_dict(),\n",
    "            'fc': self.fc.state_dict()\n",
    "        }, file_path)\n",
    "        print(f\"✅ Saved Siamese weights to {file_path}\")\n",
    "\n",
    "    def train_model(self, data_loader, epochs): # The training methods are included in the other file of Siamese Model\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25e745fa-3f61-44ff-a2e0-fd5091076e8e",
    "_uuid": "3470e1bf-3c67-4e0b-b30b-bad3bdc8b28a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:06.951692Z",
     "iopub.status.busy": "2025-04-18T10:26:06.951509Z",
     "iopub.status.idle": "2025-04-18T10:26:06.964986Z",
     "shell.execute_reply": "2025-04-18T10:26:06.964377Z",
     "shell.execute_reply.started": "2025-04-18T10:26:06.951678Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_from_xywhn_pil(image_pil, xywhn_box, padding=0):\n",
    "    \"\"\"Crop the image at the chosen box\"\"\"\n",
    "    # Extract the coordinate from the box\n",
    "    W, H = image_pil.size\n",
    "    x_center, y_center, w, h = map(float, xywhn_box)\n",
    "\n",
    "    x_c, y_c = x_center * W, y_center * H\n",
    "    bw, bh = w * W, h * H\n",
    "    \n",
    "    x1 = max(int(x_c - bw / 2) - padding, 0)\n",
    "    y1 = max(int(y_c - bh / 2) - padding, 0)\n",
    "    x2 = min(int(x_c + bw / 2) + padding, W)\n",
    "    y2 = min(int(y_c + bh / 2) + padding, H)\n",
    "\n",
    "    # Crop out the image\n",
    "    cropped = image_pil.crop((x1, y1, x2, y2))\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Define the full pipeline of Detection + Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "05cb4f29-c996-4bcb-8c48-258f6cbd76c6",
    "_uuid": "524700c0-bc30-48fd-8bba-c903597a83e0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:06.967048Z",
     "iopub.status.busy": "2025-04-18T10:26:06.966685Z",
     "iopub.status.idle": "2025-04-18T10:26:06.984779Z",
     "shell.execute_reply": "2025-04-18T10:26:06.984108Z",
     "shell.execute_reply.started": "2025-04-18T10:26:06.967030Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DetectToVerify:\n",
    "    \"\"\"\n",
    "    A unified pipeline that performs both signature detection and signature verification.\n",
    "    \n",
    "    Components:\n",
    "    - YOLO11 object detector to find signature bounding boxes in a document image.\n",
    "    - Siamese neural network to verify whether the detected signature matches a reference sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, detector_filename, verifier_filename, repo_id=None, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the detection and verification pipeline.\n",
    "\n",
    "        Args:\n",
    "            detector_filename (str): Path to the YOLO detector model (.pt file).\n",
    "            verifier_filename (str): Path to the Siamese verification model (.pt file).\n",
    "            repo_id (str, optional): Hugging Face Hub repo ID to download models if not present locally.\n",
    "            threshold (float): Distance threshold below which a match is considered genuine.\n",
    "        \"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Load YOLO signature detector model\n",
    "        if not os.path.isfile(detector_filename):\n",
    "            if repo_id is None:\n",
    "                raise Exception(\"Please pass in repo_id to pull the model\")\n",
    "            print(f\"Download {detector_filename} from hub\")\n",
    "            hf_hub_download(repo_id=repo_id, filename=detector_filename, local_dir=LOCAL_DIR)\n",
    "        self.detector = YOLO(detector_filename)\n",
    "        self.detector.to(self.device)\n",
    "        print(f\"✅ Loaded detector from {detector_filename}\\n\")\n",
    "\n",
    "        # Load Siamese signature verifier model\n",
    "        if not os.path.isfile(verifier_filename):\n",
    "            if repo_id is None:\n",
    "                raise Exception(\"Please pass in repo_id to pull the model\")\n",
    "            print(f\"Download {verifier_filename} from hub\")\n",
    "            hf_hub_download(repo_id=repo_id, filename=verifier_filename, local_dir=LOCAL_DIR)\n",
    "        self.verifier = SiameseModel(from_file=verifier_filename)\n",
    "        self.verifier.to(self.device)\n",
    "        self.verifier.eval() # Set the model to evaluate state\n",
    "        print(f\"✅ Loaded verifier from {verifier_filename}\\n\")\n",
    "\n",
    "        # Preprocessing pipeline for both the sample and cropped signature images (verify process)\n",
    "        self.transform = T.Compose([\n",
    "            T.Grayscale(),\n",
    "            T.Resize(SIGN_IMG_SIZE),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "\n",
    "    def infer(self, sample_image, document_image, min_conf=0.5, show=False):\n",
    "        \"\"\"\n",
    "        Performs inference by detecting signatures in the document and verifying each against the sample.\n",
    "\n",
    "        Args:\n",
    "            sample_image (str or PIL.Image): The reference signature image (ground truth).\n",
    "            document_image (str or PIL.Image): The document containing potential signatures.\n",
    "            min_conf (float): The mininum confidence for the detected bboxes.\n",
    "            show (bool): Whether to visualize each detected signature and distance score.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if any detected signature is verified as genuine, False otherwise.\n",
    "        \"\"\"\n",
    "        # Load and prepare sample and document images\n",
    "        if not isinstance(sample_image, Image.Image):\n",
    "            sample_image = Image.open(sample_image).convert(\"RGB\")\n",
    "        if not isinstance(document_image, Image.Image):\n",
    "            document_image = Image.open(document_image).convert(\"RGB\")\n",
    "        sample_image_tensor = self.transform(sample_image).unsqueeze(0).to(self.device)  # Prepare the sample image tensor\n",
    "        \n",
    "\n",
    "        # Run signature detection with YOLO\n",
    "        results = self.detector.predict(document_image, verbose=False, conf=min_conf)[0]\n",
    "        labels = [False] # Start with False to handle case with no detections\n",
    "\n",
    "        # Iterate through all detections\n",
    "        for box, cls, conf in zip(results.boxes.xywhn, results.boxes.cls, results.boxes.conf):\n",
    "            if int(cls) != 0:\n",
    "                continue   # Skip non-signature detections\n",
    "    \n",
    "            # Crop out the detected signature and transform it\n",
    "            cropped = crop_from_xywhn_pil(document_image, box)\n",
    "            cropped_tensor = self.transform(cropped).unsqueeze(0).to(self.device)\n",
    "    \n",
    "            # Pass sample and cropped signature to the Siamese model\n",
    "            output_sample, output_cropped = self.verifier(sample_image_tensor, cropped_tensor)\n",
    "    \n",
    "            # Compute Euclidean distance between embeddings\n",
    "            distance = F.pairwise_distance(output_sample, output_cropped).item()\n",
    "            labels.append(distance < self.threshold)   # Mark as genuine if distance below threshold\n",
    "            \n",
    "            # Optionally visualize cropped signature with distance\n",
    "            if show:\n",
    "                # Display the cropped signature and distance\n",
    "                plt.imshow(cropped)\n",
    "                plt.title(f\"Distance={distance} | Label {distance < self.threshold}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "    \n",
    "        return any(labels)\n",
    "        # return preds\n",
    "\n",
    "    def evaluate(self, dataset, min_conf=0.5):\n",
    "        \"\"\"\n",
    "        Evaluate the signature detection + verification pipeline on a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (List[Dict]): A list of dictionaries with keys:\n",
    "                - 'sample_image': reference signature (genuine)\n",
    "                - 'document_image': document containing a signature\n",
    "                - 'label': 0 if genuine, 1 if forged\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy of the pipeline.\n",
    "        \"\"\"\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(dataset, desc=\"Evaluating\", dynamic_ncols=True) # Create the progress bar\n",
    "\n",
    "        for item in pbar:\n",
    "            sample = item['sample_signature']\n",
    "            document = item['document']\n",
    "            label = item['label'] # 0 = genuine, 1 = forged\n",
    "\n",
    "            # Pass the model through inference stage\n",
    "            prediction = self.infer(sample, document, min_conf=min_conf)\n",
    "\n",
    "            if prediction == (label == 0):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            # Live accuracy update\n",
    "            accuracy = correct / total if total > 0 else 0.0\n",
    "            pbar.set_postfix({'accuracy': f'{accuracy:.2%}'})\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        print(f\"\\n✅ Accuracy: {accuracy:.2%}\")\n",
    "        return accuracy\n",
    "\n",
    "    def evaluate_siamese(self, dataset):\n",
    "        \"\"\"\n",
    "        Evaluate only the Siamese verifier on a dataset of signature pairs.\n",
    "    \n",
    "        Args:\n",
    "            dataset (List[Dict]): A list of dictionaries with keys:\n",
    "                - 'to_verify_signature': Second signature image (genuine or forged).\n",
    "                - 'sample_signature': First signature image (genuine).\n",
    "                - 'label': 0 if genuine, 1 if forged.\n",
    "            show (bool): Whether to visualize example predictions with distances.\n",
    "    \n",
    "        Returns:\n",
    "            float: Accuracy of the verifier model.\n",
    "        \"\"\"\n",
    "        correct, total = 0, 0\n",
    "        progress_bar = tqdm(dataset, desc=\"Evaluating Siamese Verifier\", dynamic_ncols=True) # Create the progress bar\n",
    "    \n",
    "        for item in progress_bar:\n",
    "            img1 = item['to_verify_signature']\n",
    "            img2 = item['sample_signature']\n",
    "            label = item['label']\n",
    "    \n",
    "            # Load and transform both images, send to the correct device of cuda or cpu\n",
    "            if not isinstance(img1, Image.Image):\n",
    "                img1 = Image.open(img1).convert(\"RGB\")\n",
    "            if not isinstance(img2, Image.Image):\n",
    "                img2 = Image.open(img2).convert(\"RGB\")\n",
    "                \n",
    "            tensor1 = self.transform(img1).unsqueeze(0).to(self.device)\n",
    "            tensor2 = self.transform(img2).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Run prediction without updating the weights\n",
    "            with torch.no_grad():\n",
    "                out1, out2 = self.verifier(tensor1, tensor2)\n",
    "                distance = F.pairwise_distance(out1, out2).item()\n",
    "                prediction = 0 if distance < self.threshold else 1  # 0 = genuine, 1 = forged\n",
    "    \n",
    "            if prediction == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "            # Live accuracy update\n",
    "            accuracy = correct / total if total > 0 else 0.0\n",
    "            progress_bar.set_postfix({'accuracy': f'{accuracy:.2%}'})  # Create the progress bar\n",
    "\n",
    "        final_acc = correct / total if total > 0 else 0.0\n",
    "        print(f\"\\n✅ Siamese Verifier Final Accuracy: {final_acc:.4%}\")\n",
    "        return final_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Import the dataset and prepare the model (both pulling from HUB)\n",
    "\n",
    "***Dataset***  \n",
    "We are using our improvised dataset, storing at [Mels22/SigDetectVerifyFlow](https://huggingface.co/datasets/Mels22/SigDetectVerifyFlow). Describing the dataset within the few words, it allows the full flow from Detection to Verification of signatures.\n",
    "\n",
    "Each sample in the dataset contains the following fields:\n",
    "\n",
    "- `document` *(Image)*: The full document image that contains one or more handwritten signatures.\n",
    "- `bbox` *(List of Bounding Boxes)*: The coordinates of the signature(s) detected in the `document`. Format: `[x_min, y_min, x_max, y_max]`.\n",
    "- `to_verify_signature` *(Image)*: A cropped signature from the document image that needs to be verified.\n",
    "- `sample_signature` *(Image)*: A standard reference signature used for comparison.\n",
    "- `label` *(int)*: Indicates if the `to_verify_signature` is **genuine (0)** or **forged (1)** when compared to the `sample_signature`.\n",
    "\n",
    "___\n",
    "***Model***\n",
    "We store our trained model on the [Mels22/Signature-Detection-Verification](https://huggingface.co/Mels22/Signature-Detection-Verification), which in short, contains the following files:\n",
    "- `detector_yolo_1cls.pt`: The detection model to be trained on recognizing `signature` only.\n",
    "- `detector_yolo_4cls.pt`: The detection model to be trained on recognizing hand-written elements: `signature`, `initial`, `redaction`, and `date`.\n",
    "- `verifier_siamese.pt`: The verification model with the above defined architecture to classify the 2 images as genuine or forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:06.985546Z",
     "iopub.status.busy": "2025-04-18T10:26:06.985310Z",
     "iopub.status.idle": "2025-04-18T10:26:07.687282Z",
     "shell.execute_reply": "2025-04-18T10:26:07.686734Z",
     "shell.execute_reply.started": "2025-04-18T10:26:06.985531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "LOCAL_DIR = \".\" # Save the downloaded model to \n",
    "\n",
    "# Model and dataset repo on Hugging Face Hub\n",
    "MODEL_REPO = \"Mels22/Signature-Detection-Verification\"\n",
    "DATASET_REPO = \"Mels22/SigDetectVerifyFlow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9133baf-664a-4c24-b187-0f81dd60e9e7",
    "_uuid": "5c7d218f-a16c-42b2-bad9-e82826f15267",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:07.688482Z",
     "iopub.status.busy": "2025-04-18T10:26:07.687959Z",
     "iopub.status.idle": "2025-04-18T10:26:08.702306Z",
     "shell.execute_reply": "2025-04-18T10:26:08.701510Z",
     "shell.execute_reply.started": "2025-04-18T10:26:07.688448Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_loader = load_dataset(DATASET_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1d01314-48d8-440a-b862-6a7b85bed3d9",
    "_uuid": "2729c063-33b7-46d5-96ab-a70bc2e4f034",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:08.703437Z",
     "iopub.status.busy": "2025-04-18T10:26:08.703149Z",
     "iopub.status.idle": "2025-04-18T10:26:09.057833Z",
     "shell.execute_reply": "2025-04-18T10:26:09.057134Z",
     "shell.execute_reply.started": "2025-04-18T10:26:08.703419Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flow = DetectToVerify(\n",
    "    detector_filename=\"detector_yolo_1cls.pt\", # Path to detection model\n",
    "    verifier_filename=\"verifier_siamese.pt\", # Path to verification model\n",
    "    repo_id=MODEL_REPO,\n",
    "    threshold=0.5, # Threshold for siamese verification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Inference / Evaluate the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T10:26:09.058913Z",
     "iopub.status.busy": "2025-04-18T10:26:09.058646Z",
     "iopub.status.idle": "2025-04-18T10:30:10.452334Z",
     "shell.execute_reply": "2025-04-18T10:30:10.451535Z",
     "shell.execute_reply.started": "2025-04-18T10:26:09.058890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flow.evaluate(data_loader['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 107946,
     "sourceId": 262177,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7159345,
     "sourceId": 11459247,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
